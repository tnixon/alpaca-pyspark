{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Alpaca PySpark Connector Demonstration\n",
    "\n",
    "This notebook demonstrates how to use the Alpaca Historical Bars PySpark Connector to fetch and analyze stock market data using distributed computation.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Alpaca API Credentials**: You need an Alpaca account and API credentials\n",
    "   - Sign up at [Alpaca](https://alpaca.markets/)\n",
    "   - Generate API keys from your dashboard\n",
    "   - Set environment variables `ALPACA_API_KEY` and `ALPACA_SECRET_KEY`\n",
    "\n",
    "2. **Required Libraries**: This notebook requires PySpark, requests, and visualization libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in a fresh environment)\n",
    "# !pip install pyspark requests matplotlib seaborn pandas\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Suppress Spark warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql_2.12:3.5.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup Spark Session and Connector\n",
    "\n",
    "First, we'll initialize a Spark session and create our Alpaca connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.alpaca_connector import create_connector\n",
    "\n",
    "# Create Spark session with optimized configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AlpacaHistoricalBarsDemo\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Number of cores available: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Alpaca connector\n",
    "# Note: Make sure ALPACA_API_KEY and ALPACA_SECRET_KEY environment variables are set\n",
    "# For demonstration purposes, you can also pass them directly:\n",
    "# connector = create_connector(spark, api_key=\"your_key\", api_secret=\"your_secret\")\n",
    "\n",
    "connector = create_connector(\n",
    "    spark,\n",
    "    # Optional: customize configuration\n",
    "    page_size=10000,  # Maximum records per API call\n",
    "    date_split_days=30,  # Split date ranges into chunks of this many days\n",
    "    max_retries=3,  # Number of retry attempts for failed requests\n",
    "    timeout=30  # Request timeout in seconds\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "if connector.validate_connection():\n",
    "    print(\"‚úÖ Successfully connected to Alpaca API\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to Alpaca API\")\n",
    "    print(\"Please check your API credentials and network connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Fetching Historical Data\n",
    "\n",
    "Let's fetch historical data for some popular stocks and demonstrate the distributed processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stocks we want to analyze\n",
    "symbols = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']\n",
    "\n",
    "# Define date range (last 3 months)\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.now() - timedelta(days=90)).strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Fetching data for {len(symbols)} symbols from {start_date} to {end_date}\")\n",
    "print(f\"Symbols: {', '.join(symbols)}\")\n",
    "\n",
    "# Fetch the data using distributed processing\n",
    "df = connector.get_historical_bars(\n",
    "    symbols=symbols,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    timeframe='1Day'  # Daily bars\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Retrieved {df.count()} total bars\")\n",
    "print(f\"üìÖ Date range: {df.agg({'timestamp': 'min'}).collect()[0][0]} to {df.agg({'timestamp': 'max'}).collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics by symbol\n",
    "print(\"üìà Basic Statistics by Symbol:\")\n",
    "stats_df = df.groupBy(\"symbol\").agg(\n",
    "    {\"volume\": \"sum\", \"close\": \"avg\", \"high\": \"max\", \"low\": \"min\", \"*\": \"count\"}\n",
    ").withColumnRenamed(\"count(1)\", \"days_traded\")\n",
    "\n",
    "stats_df.orderBy(\"symbol\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization\n",
    "\n",
    "Now let's perform some analysis and create visualizations of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for easier visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert Spark DataFrame to Pandas (be careful with large datasets)\n",
    "pandas_df = df.orderBy(\"symbol\", \"timestamp\").toPandas()\n",
    "\n",
    "# Convert timestamp to datetime if it's not already\n",
    "pandas_df['timestamp'] = pd.to_datetime(pandas_df['timestamp'])\n",
    "\n",
    "print(f\"Converted {len(pandas_df)} rows to Pandas DataFrame\")\n",
    "print(f\"Memory usage: {pandas_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# Create a comprehensive dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "fig.suptitle('Alpaca Historical Data Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Price trends over time\n",
    "ax1 = axes[0, 0]\n",
    "for symbol in symbols:\n",
    "    symbol_data = pandas_df[pandas_df['symbol'] == symbol]\n",
    "    ax1.plot(symbol_data['timestamp'], symbol_data['close'], label=symbol, linewidth=2)\n",
    "\n",
    "ax1.set_title('Stock Price Trends (Close)', fontweight='bold')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Volume comparison\n",
    "ax2 = axes[0, 1]\n",
    "volume_by_symbol = pandas_df.groupby('symbol')['volume'].sum().sort_values(ascending=True)\n",
    "bars = ax2.barh(volume_by_symbol.index, volume_by_symbol.values / 1e6)  # Convert to millions\n",
    "ax2.set_title('Total Trading Volume (3 Months)', fontweight='bold')\n",
    "ax2.set_xlabel('Volume (Millions)')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width + width*0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.0f}M', ha='left', va='center')\n",
    "\n",
    "# 3. Price volatility (daily returns)\n",
    "ax3 = axes[1, 0]\n",
    "returns_data = []\n",
    "for symbol in symbols:\n",
    "    symbol_data = pandas_df[pandas_df['symbol'] == symbol].sort_values('timestamp')\n",
    "    daily_returns = symbol_data['close'].pct_change().dropna() * 100\n",
    "    returns_data.append(daily_returns.values)\n",
    "\n",
    "ax3.boxplot(returns_data, labels=symbols)\n",
    "ax3.set_title('Daily Returns Distribution (%)', fontweight='bold')\n",
    "ax3.set_xlabel('Symbol')\n",
    "ax3.set_ylabel('Daily Return (%)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Average daily range (High - Low)\n",
    "ax4 = axes[1, 1]\n",
    "pandas_df['daily_range_pct'] = ((pandas_df['high'] - pandas_df['low']) / pandas_df['close']) * 100\n",
    "avg_range = pandas_df.groupby('symbol')['daily_range_pct'].mean().sort_values()\n",
    "\n",
    "bars = ax4.bar(avg_range.index, avg_range.values, color='skyblue', alpha=0.7)\n",
    "ax4.set_title('Average Daily Price Range (%)', fontweight='bold')\n",
    "ax4.set_xlabel('Symbol')\n",
    "ax4.set_ylabel('Average Daily Range (%)')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Advanced Analysis with PySpark\n",
    "\n",
    "Let's perform some advanced analysis using PySpark's distributed computing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag, when, abs as spark_abs, avg, stddev, max as spark_max, min as spark_min\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window specifications for technical analysis\n",
    "window_spec = Window.partitionBy(\"symbol\").orderBy(\"timestamp\")\n",
    "\n",
    "# Calculate technical indicators\n",
    "technical_df = df.withColumn(\n",
    "    \"prev_close\", lag(\"close\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"daily_return_pct\", \n",
    "    when(col(\"prev_close\").isNotNull(), \n",
    "         ((col(\"close\") - col(\"prev_close\")) / col(\"prev_close\")) * 100\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"daily_range_pct\",\n",
    "    ((col(\"high\") - col(\"low\")) / col(\"close\")) * 100\n",
    ")\n",
    "\n",
    "print(\"Technical indicators calculated. Sample data:\")\n",
    "technical_df.select(\"symbol\", \"timestamp\", \"close\", \"daily_return_pct\", \"daily_range_pct\") \\\n",
    "    .filter(col(\"daily_return_pct\") != 0) \\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling averages using window functions\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Define rolling windows\n",
    "window_5d = Window.partitionBy(\"symbol\").orderBy(\"timestamp\").rowsBetween(-4, 0)\n",
    "window_20d = Window.partitionBy(\"symbol\").orderBy(\"timestamp\").rowsBetween(-19, 0)\n",
    "\n",
    "# Add moving averages\n",
    "ma_df = technical_df.withColumn(\n",
    "    \"ma_5\", avg(\"close\").over(window_5d)\n",
    ").withColumn(\n",
    "    \"ma_20\", avg(\"close\").over(window_20d)\n",
    ")\n",
    "\n",
    "# Find trading signals (when 5-day MA crosses above 20-day MA)\n",
    "signals_df = ma_df.withColumn(\n",
    "    \"prev_ma_5\", lag(\"ma_5\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"prev_ma_20\", lag(\"ma_20\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"golden_cross\",\n",
    "    when(\n",
    "        (col(\"prev_ma_5\") <= col(\"prev_ma_20\")) & (col(\"ma_5\") > col(\"ma_20\")),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# Show golden cross signals\n",
    "golden_crosses = signals_df.filter(col(\"golden_cross\") == True) \\\n",
    "    .select(\"symbol\", \"timestamp\", \"close\", \"ma_5\", \"ma_20\")\n",
    "\n",
    "print(\"üéØ Golden Cross Signals (5-day MA crossing above 20-day MA):\")\n",
    "golden_crosses.show(truncate=False)\n",
    "print(f\"Total signals found: {golden_crosses.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk analysis: Calculate volatility metrics\n",
    "risk_metrics = technical_df.filter(col(\"daily_return_pct\") != 0) \\\n",
    "    .groupBy(\"symbol\") \\\n",
    "    .agg(\n",
    "        avg(\"daily_return_pct\").alias(\"avg_daily_return\"),\n",
    "        stddev(\"daily_return_pct\").alias(\"volatility\"),\n",
    "        spark_max(\"daily_return_pct\").alias(\"max_daily_gain\"),\n",
    "        spark_min(\"daily_return_pct\").alias(\"max_daily_loss\"),\n",
    "        avg(\"volume\").alias(\"avg_volume\")\n",
    "    )\n",
    "\n",
    "print(\"üìä Risk Metrics Analysis:\")\n",
    "risk_metrics.orderBy(\"volatility\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's analyze the performance of our distributed data loading approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test with different configurations to show scalability\n",
    "print(\"üöÄ Performance Testing: Different Configuration Settings\\n\")\n",
    "\n",
    "test_symbols = ['AAPL', 'GOOGL']\n",
    "test_start = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "test_end = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Test 1: Larger date chunks (less parallel tasks)\n",
    "print(\"Test 1: Larger date chunks (30 days per chunk)\")\n",
    "connector_large_chunks = create_connector(spark, date_split_days=30)\n",
    "\n",
    "start_time = time.time()\n",
    "df_large = connector_large_chunks.get_historical_bars(test_symbols, test_start, test_end)\n",
    "count_large = df_large.count()\n",
    "time_large = time.time() - start_time\n",
    "\n",
    "print(f\"   Results: {count_large} bars in {time_large:.2f} seconds\")\n",
    "\n",
    "# Test 2: Smaller date chunks (more parallel tasks)\n",
    "print(\"\\nTest 2: Smaller date chunks (7 days per chunk)\")\n",
    "connector_small_chunks = create_connector(spark, date_split_days=7)\n",
    "\n",
    "start_time = time.time()\n",
    "df_small = connector_small_chunks.get_historical_bars(test_symbols, test_start, test_end)\n",
    "count_small = df_small.count()\n",
    "time_small = time.time() - start_time\n",
    "\n",
    "print(f\"   Results: {count_small} bars in {time_small:.2f} seconds\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nüìà Performance Summary:\")\n",
    "print(f\"   Large chunks: {time_large:.2f}s\")\n",
    "print(f\"   Small chunks: {time_small:.2f}s\")\n",
    "if time_small < time_large:\n",
    "    improvement = ((time_large - time_small) / time_large) * 100\n",
    "    print(f\"   ‚úÖ Smaller chunks were {improvement:.1f}% faster\")\n",
    "else:\n",
    "    difference = ((time_small - time_large) / time_large) * 100\n",
    "    print(f\"   üìä Larger chunks were {difference:.1f}% faster\")\n",
    "\n",
    "print(f\"\\nNote: Performance depends on network conditions, API rate limits, and cluster resources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Exporting Results\n",
    "\n",
    "Finally, let's export our analysis results to different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Parquet (efficient for big data)\n",
    "print(\"üíæ Exporting data to various formats...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export main dataset to Parquet\n",
    "parquet_path = f\"{output_dir}/historical_bars.parquet\"\n",
    "df.coalesce(1).write.mode(\"overwrite\").parquet(parquet_path)\n",
    "print(f\"‚úÖ Main dataset exported to: {parquet_path}\")\n",
    "\n",
    "# Export risk metrics to CSV\n",
    "csv_path = f\"{output_dir}/risk_metrics.csv\"\n",
    "risk_metrics.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)\n",
    "print(f\"‚úÖ Risk metrics exported to: {csv_path}\")\n",
    "\n",
    "# Export golden cross signals to JSON\n",
    "json_path = f\"{output_dir}/golden_cross_signals.json\"\n",
    "golden_crosses.coalesce(1).write.mode(\"overwrite\").json(json_path)\n",
    "print(f\"‚úÖ Trading signals exported to: {json_path}\")\n",
    "\n",
    "print(f\"\\nüìÅ All files saved in: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated the key features of the Alpaca PySpark Connector:\n",
    "\n",
    "### ‚úÖ What we accomplished:\n",
    "1. **Connected to Alpaca API** using secure credential management\n",
    "2. **Fetched historical data** for multiple symbols using distributed processing\n",
    "3. **Performed technical analysis** with PySpark's window functions\n",
    "4. **Created visualizations** showing price trends, volume, and volatility\n",
    "5. **Identified trading signals** using moving average crossovers\n",
    "6. **Analyzed performance** with different configuration settings\n",
    "7. **Exported results** to multiple formats (Parquet, CSV, JSON)\n",
    "\n",
    "### üîß Key Features:\n",
    "- **Distributed Loading**: Automatic parallelization across date ranges and symbols\n",
    "- **Flexible Configuration**: Customizable page sizes, retry logic, and chunking strategies  \n",
    "- **Error Handling**: Robust retry mechanisms and rate limit handling\n",
    "- **Multiple Formats**: Support for various timeframes (1Min, 5Min, 1Day, etc.)\n",
    "- **Scalability**: Efficient processing of large datasets using Spark\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Scale up**: Try with more symbols and longer date ranges\n",
    "2. **Real-time integration**: Combine with streaming data sources\n",
    "3. **Advanced analytics**: Implement more sophisticated trading strategies\n",
    "4. **Model training**: Use the data for machine learning models\n",
    "5. **Production deployment**: Set up automated data pipelines\n",
    "\n",
    "### üìö Resources:\n",
    "- [Alpaca API Documentation](https://docs.alpaca.markets/)\n",
    "- [PySpark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [Connector Source Code](../src/alpaca_connector.py)\n",
    "- [Test Suite](../tests/test_alpaca_connector.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Spark session\n",
    "print(\"üßπ Cleaning up Spark session...\")\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session stopped successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}