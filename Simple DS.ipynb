{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df94f89c-2702-4a05-a271-430bc2bd705f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from pyspark.sql.datasource import DataSource, DataSourceReader\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "class SimpleDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    A simple data source for PySpark that generates exactly two rows of synthetic data.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return \"simple\"\n",
    "\n",
    "    def schema(self):\n",
    "        return \"\"\"\n",
    "            symbol STRING,\n",
    "            time TIMESTAMP,\n",
    "            open FLOAT,\n",
    "            high FLOAT,\n",
    "            low FLOAT,\n",
    "            close FLOAT,\n",
    "            volume INT,\n",
    "            trade_count INT,\n",
    "            vwap FLOAT\n",
    "        \"\"\"\n",
    "\n",
    "    def reader(self, schema: StructType):\n",
    "        return SimpleDataSourceReader()\n",
    "\n",
    "class SimpleDataSourceReader(DataSourceReader):\n",
    "\n",
    "    def read(self, partition):\n",
    "        test_bars = [\n",
    "            ('AAPL', dt.fromisoformat('2021-01-04T05:00:00Z'), 133.52, 133.6116, 126.76, 129.41, 158211374, 1310229, 129.717982),\n",
    "            ('AAPL', dt.fromisoformat('2021-01-05T05:00:00Z'), 128.89, 131.74, 128.43, 131.01, 105863439, 707583, 130.738233),\n",
    "            ('AAPL', dt.fromisoformat('2021-01-06T05:00:00Z'), 127.72, 131.0499, 126.382, 126.66, 165568781, 1202579, 128.249403),\n",
    "            ('AAPL', dt.fromisoformat('2021-01-07T05:00:00Z'), 128.36, 131.63, 127.86, 130.92, 118743769, 718362, 130.185457),\n",
    "            ('AAPL', dt.fromisoformat('2021-01-08T05:00:00Z'), 132.43, 132.63, 130.23, 132.05, 112696090, 798393, 131.580087)\n",
    "        ]\n",
    "        for bar in test_bars:\n",
    "            yield bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77606d98-f4bc-4c83-88a4-96ec6e673e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.dataSource.register(SimpleDataSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bec5069-30ac-409f-abac-7caa23c592f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"simple\").load().show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Simple DS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
