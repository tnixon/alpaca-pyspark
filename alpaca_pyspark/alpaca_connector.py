"""
Alpaca Historical Bars PySpark Connector

A PySpark connector for fetching historical stock bars from the Alpaca API
with distributed computation capabilities.

Author: Generated by Claude
License: MIT
"""

import os
import logging
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, Tuple
import json

import requests
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType
from pyspark.sql.functions import col, to_timestamp
from pyspark import TaskContext


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AlpacaHistoricalBarsConnector:
    """
    PySpark connector for fetching historical stock bars from Alpaca API.
    
    This connector uses Spark's distributed computation model to fetch data
    in parallel across different date ranges and symbols.
    """
    
    # Alpaca API base URL for stock bars
    BASE_URL = "https://data.alpaca.markets/v2/stocks/bars"
    
    # Default configuration
    DEFAULT_CONFIG = {
        'page_size': 10000,  # Maximum allowed by Alpaca
        'max_retries': 3,
        'timeout': 30,
        'date_split_days': 30,  # Split requests into chunks of this many days
        'timeframe': '1Day',
    }
    
    # Schema for the returned DataFrame
    BARS_SCHEMA = StructType([
        StructField("symbol", StringType(), False),
        StructField("timestamp", TimestampType(), False),
        StructField("open", DoubleType(), True),
        StructField("high", DoubleType(), True),
        StructField("low", DoubleType(), True),
        StructField("close", DoubleType(), True),
        StructField("volume", LongType(), True),
        StructField("trade_count", LongType(), True),
        StructField("vwap", DoubleType(), True)
    ])
    
    def __init__(self, spark: SparkSession, api_key: Optional[str] = None, 
                 api_secret: Optional[str] = None, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the Alpaca connector.
        
        Args:
            spark: SparkSession instance
            api_key: Alpaca API key (or set ALPACA_API_KEY env var)
            api_secret: Alpaca API secret (or set ALPACA_SECRET_KEY env var)
            config: Additional configuration options
        """
        self.spark = spark
        self.config = {**self.DEFAULT_CONFIG, **(config or {})}
        
        # Get API credentials
        self.api_key = api_key or os.getenv('ALPACA_API_KEY')
        self.api_secret = api_secret or os.getenv('ALPACA_SECRET_KEY')
        
        if not self.api_key or not self.api_secret:
            raise ValueError(
                "API credentials required. Set ALPACA_API_KEY and ALPACA_SECRET_KEY "
                "environment variables or pass them to constructor."
            )
        
        self.headers = {
            'APCA-API-KEY-ID': self.api_key,
            'APCA-API-SECRET-KEY': self.api_secret,
            'Content-Type': 'application/json'
        }
    
    def get_historical_bars(self, symbols: List[str], start_date: str, 
                           end_date: str, timeframe: Optional[str] = None) -> DataFrame:
        """
        Fetch historical bars for given symbols and date range.
        
        Args:
            symbols: List of stock symbols (e.g., ['AAPL', 'GOOGL'])
            start_date: Start date in 'YYYY-MM-DD' format
            end_date: End date in 'YYYY-MM-DD' format
            timeframe: Bar timeframe (1Min, 5Min, 15Min, 30Min, 1Hour, 1Day)
        
        Returns:
            PySpark DataFrame with historical bars data
        """
        timeframe = timeframe or self.config['timeframe']
        
        # Generate date ranges for parallel processing
        date_ranges = self._generate_date_ranges(start_date, end_date)
        
        # Create RDD of (symbol, start_date, end_date) tuples for parallel processing
        tasks = []
        for symbol in symbols:
            for start, end in date_ranges:
                tasks.append((symbol, start, end, timeframe))
        
        logger.info(f"Generated {len(tasks)} tasks for parallel processing")
        
        # Distribute tasks across Spark cluster
        tasks_rdd = self.spark.sparkContext.parallelize(tasks, numSlices=len(tasks))
        
        # Fetch data in parallel
        results_rdd = tasks_rdd.flatMap(lambda task: self._fetch_bars_for_range(*task))
        
        # Convert to DataFrame
        if results_rdd.isEmpty():
            logger.warning("No data retrieved")
            return self.spark.createDataFrame([], schema=self.BARS_SCHEMA)
        
        df = self.spark.createDataFrame(results_rdd, schema=self.BARS_SCHEMA)
        
        # Sort by symbol and timestamp
        df = df.orderBy("symbol", "timestamp")
        
        logger.info(f"Successfully retrieved {df.count()} bars")
        return df
    
    def _generate_date_ranges(self, start_date: str, end_date: str) -> List[Tuple[str, str]]:
        """Generate date ranges for parallel processing."""
        start = datetime.strptime(start_date, '%Y-%m-%d')
        end = datetime.strptime(end_date, '%Y-%m-%d')
        
        ranges = []
        current = start
        split_days = self.config['date_split_days']
        
        while current < end:
            range_end = min(current + timedelta(days=split_days), end)
            ranges.append((
                current.strftime('%Y-%m-%d'),
                range_end.strftime('%Y-%m-%d')
            ))
            current = range_end + timedelta(days=1)
        
        return ranges
    
    def _fetch_bars_for_range(self, symbol: str, start_date: str, 
                             end_date: str, timeframe: str) -> List[Dict[str, Any]]:
        """
        Fetch bars for a single symbol and date range.
        This method runs on Spark workers.
        """
        try:
            # Get task context for logging
            task_context = TaskContext.get()
            partition_id = task_context.partitionId() if task_context else 0
            
            logger.info(f"Partition {partition_id}: Fetching {symbol} from {start_date} to {end_date}")
            
            all_bars = []
            next_page_token = None
            
            while True:
                params = {
                    'symbols': symbol,
                    'timeframe': timeframe,
                    'start': start_date,
                    'end': end_date,
                    'limit': self.config['page_size'],
                    'adjustment': 'raw',
                    'feed': 'iex'  # Use IEX feed for free data
                }
                
                if next_page_token:
                    params['page_token'] = next_page_token
                
                # Make request with retry logic
                response = self._make_request_with_retry(params)
                
                if not response:
                    break
                
                # Extract bars from response
                bars_data = response.get('bars', {}).get(symbol, [])
                
                for bar in bars_data:
                    # Convert timestamp to datetime
                    timestamp = datetime.fromisoformat(bar['t'].replace('Z', '+00:00'))
                    
                    all_bars.append({
                        'symbol': symbol,
                        'timestamp': timestamp,
                        'open': float(bar['o']),
                        'high': float(bar['h']),
                        'low': float(bar['l']),
                        'close': float(bar['c']),
                        'volume': int(bar['v']),
                        'trade_count': int(bar.get('n', 0)),
                        'vwap': float(bar.get('vw', 0.0))
                    })
                
                # Check for next page
                next_page_token = response.get('next_page_token')
                if not next_page_token:
                    break
            
            logger.info(f"Partition {partition_id}: Retrieved {len(all_bars)} bars for {symbol}")
            return all_bars
            
        except Exception as e:
            logger.error(f"Error fetching data for {symbol}: {str(e)}")
            return []
    
    def _make_request_with_retry(self, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Make HTTP request with retry logic."""
        for attempt in range(self.config['max_retries']):
            try:
                response = requests.get(
                    self.BASE_URL,
                    params=params,
                    headers=self.headers,
                    timeout=self.config['timeout']
                )
                
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:  # Rate limited
                    logger.warning(f"Rate limited, attempt {attempt + 1}")
                    if attempt < self.config['max_retries'] - 1:
                        import time
                        time.sleep(2 ** attempt)  # Exponential backoff
                        continue
                else:
                    logger.error(f"API error {response.status_code}: {response.text}")
                    break
                    
            except Exception as e:
                logger.error(f"Request error (attempt {attempt + 1}): {str(e)}")
                if attempt < self.config['max_retries'] - 1:
                    import time
                    time.sleep(1)
                    continue
                    
        return None
    
    def validate_connection(self) -> bool:
        """Test the connection to Alpaca API."""
        try:
            # Test with a simple request
            params = {
                'symbols': 'AAPL',
                'timeframe': '1Day',
                'start': '2024-01-01',
                'end': '2024-01-02',
                'limit': 1
            }
            
            response = requests.get(
                self.BASE_URL,
                params=params,
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 200:
                logger.info("Connection to Alpaca API successful")
                return True
            else:
                logger.error(f"Connection failed: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"Connection test failed: {str(e)}")
            return False


def create_connector(spark: SparkSession, api_key: Optional[str] = None, 
                    api_secret: Optional[str] = None, **config) -> AlpacaHistoricalBarsConnector:
    """
    Convenience function to create an Alpaca connector.
    
    Args:
        spark: SparkSession instance
        api_key: Alpaca API key (optional if env var set)
        api_secret: Alpaca API secret (optional if env var set)
        **config: Additional configuration options
    
    Returns:
        AlpacaHistoricalBarsConnector instance
    """
    return AlpacaHistoricalBarsConnector(spark, api_key, api_secret, config)


if __name__ == "__main__":
    # Example usage
    spark = SparkSession.builder \
        .appName("AlpacaHistoricalBarsExample") \
        .getOrCreate()
    
    try:
        # Create connector
        connector = create_connector(spark)
        
        # Validate connection
        if not connector.validate_connection():
            print("Failed to connect to Alpaca API")
            exit(1)
        
        # Fetch historical data
        symbols = ['AAPL', 'GOOGL']
        start_date = '2024-01-01'
        end_date = '2024-01-31'
        
        df = connector.get_historical_bars(symbols, start_date, end_date)
        
        # Show results
        print(f"Retrieved {df.count()} bars")
        df.show(20)
        
        # Basic statistics
        df.groupBy("symbol").agg(
            {"volume": "sum", "close": "avg", "high": "max", "low": "min"}
        ).show()
        
    finally:
        spark.stop()