"""
Tests for Alpaca Historical Bars PySpark Connector

Comprehensive test suite for the AlpacaHistoricalBarsConnector.

Author: Generated by Claude
License: MIT
"""

import unittest
from unittest.mock import Mock, patch, MagicMock
import os
from datetime import datetime, timedelta
import json

import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType

from alpaca_pyspark.alpaca_connector import AlpacaHistoricalBarsConnector, create_connector


class TestAlpacaHistoricalBarsConnector(unittest.TestCase):
    """Test suite for AlpacaHistoricalBarsConnector."""
    
    @classmethod
    def setUpClass(cls):
        """Set up SparkSession for all tests."""
        cls.spark = SparkSession.builder \
            .appName("AlpacaConnectorTests") \
            .master("local[2]") \
            .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse") \
            .getOrCreate()
        cls.spark.sparkContext.setLogLevel("WARN")
    
    @classmethod
    def tearDownClass(cls):
        """Tear down SparkSession."""
        cls.spark.stop()
    
    def setUp(self):
        """Set up test fixtures."""
        # Mock API credentials
        self.api_key = "test_api_key"
        self.api_secret = "test_api_secret"
        
        # Sample API response data
        self.sample_api_response = {
            "bars": {
                "AAPL": [
                    {
                        "t": "2024-01-01T09:30:00Z",
                        "o": 150.0,
                        "h": 155.0,
                        "l": 149.0,
                        "c": 154.0,
                        "v": 1000000,
                        "n": 5000,
                        "vw": 152.5
                    },
                    {
                        "t": "2024-01-02T09:30:00Z",
                        "o": 154.0,
                        "h": 156.0,
                        "l": 152.0,
                        "c": 155.0,
                        "v": 1200000,
                        "n": 5500,
                        "vw": 154.2
                    }
                ]
            },
            "next_page_token": None
        }
    
    def test_init_with_credentials(self):
        """Test connector initialization with explicit credentials."""
        connector = AlpacaHistoricalBarsConnector(
            self.spark, 
            api_key=self.api_key,
            api_secret=self.api_secret
        )
        
        self.assertEqual(connector.api_key, self.api_key)
        self.assertEqual(connector.api_secret, self.api_secret)
        self.assertEqual(connector.headers['APCA-API-KEY-ID'], self.api_key)
        self.assertEqual(connector.headers['APCA-API-SECRET-KEY'], self.api_secret)
    
    @patch.dict(os.environ, {'ALPACA_API_KEY': 'env_key', 'ALPACA_SECRET_KEY': 'env_secret'})
    def test_init_with_env_vars(self):
        """Test connector initialization with environment variables."""
        connector = AlpacaHistoricalBarsConnector(self.spark)
        
        self.assertEqual(connector.api_key, 'env_key')
        self.assertEqual(connector.api_secret, 'env_secret')
    
    def test_init_without_credentials_raises_error(self):
        """Test that missing credentials raise ValueError."""
        with patch.dict(os.environ, {}, clear=True):
            with self.assertRaises(ValueError) as context:
                AlpacaHistoricalBarsConnector(self.spark)
            
            self.assertIn("API credentials required", str(context.exception))
    
    def test_custom_config(self):
        """Test connector initialization with custom configuration."""
        custom_config = {
            'page_size': 5000,
            'max_retries': 5,
            'timeout': 60
        }
        
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret,
            config=custom_config
        )
        
        self.assertEqual(connector.config['page_size'], 5000)
        self.assertEqual(connector.config['max_retries'], 5)
        self.assertEqual(connector.config['timeout'], 60)
        # Default values should still be present
        self.assertEqual(connector.config['date_split_days'], 30)
    
    def test_generate_date_ranges_single_range(self):
        """Test date range generation for short periods."""
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret
        )
        
        ranges = connector._generate_date_ranges('2024-01-01', '2024-01-10')
        
        self.assertEqual(len(ranges), 1)
        self.assertEqual(ranges[0], ('2024-01-01', '2024-01-10'))
    
    def test_generate_date_ranges_multiple_ranges(self):
        """Test date range generation for longer periods."""
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret,
            config={'date_split_days': 10}
        )
        
        ranges = connector._generate_date_ranges('2024-01-01', '2024-01-25')
        
        self.assertEqual(len(ranges), 3)
        self.assertEqual(ranges[0], ('2024-01-01', '2024-01-11'))
        self.assertEqual(ranges[1], ('2024-01-12', '2024-01-22'))
        self.assertEqual(ranges[2], ('2024-01-23', '2024-01-25'))
    
    @patch('src.alpaca_connector.requests.get')
    def test_make_request_with_retry_success(self, mock_get):
        """Test successful API request."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = self.sample_api_response
        mock_get.return_value = mock_response
        
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret
        )
        
        params = {'symbols': 'AAPL', 'start': '2024-01-01'}
        result = connector._make_request_with_retry(params)
        
        self.assertEqual(result, self.sample_api_response)
        mock_get.assert_called_once()
    
    @patch('src.alpaca_connector.requests.get')
    def test_make_request_with_retry_failure(self, mock_get):
        """Test API request failure."""
        mock_response = Mock()
        mock_response.status_code = 400
        mock_response.text = "Bad request"
        mock_get.return_value = mock_response
        
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret
        )
        
        params = {'symbols': 'AAPL', 'start': '2024-01-01'}
        result = connector._make_request_with_retry(params)
        
        self.assertIsNone(result)
    
    @patch('src.alpaca_connector.requests.get')
    @patch('time.sleep')  # Mock sleep to speed up tests
    def test_make_request_with_retry_rate_limit(self, mock_sleep, mock_get):
        """Test API request with rate limiting."""
        # First call returns 429, second returns 200
        mock_responses = [Mock(), Mock()]
        mock_responses[0].status_code = 429
        mock_responses[1].status_code = 200
        mock_responses[1].json.return_value = self.sample_api_response
        mock_get.side_effect = mock_responses
        
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret
        )
        
        params = {'symbols': 'AAPL', 'start': '2024-01-01'}
        result = connector._make_request_with_retry(params)
        
        self.assertEqual(result, self.sample_api_response)
        self.assertEqual(mock_get.call_count, 2)
        mock_sleep.assert_called_once()
    
    def test_fetch_bars_for_range(self):
        """Test fetching bars for a single range."""
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret
        )
        
        # Mock the request method
        with patch.object(connector, '_make_request_with_retry') as mock_request:
            mock_request.return_value = self.sample_api_response
            
            result = connector._fetch_bars_for_range('AAPL', '2024-01-01', '2024-01-02', '1Day')
            
            self.assertEqual(len(result), 2)
            
            # Check first bar
            bar1 = result[0]
            self.assertEqual(bar1['symbol'], 'AAPL')
            self.assertEqual(bar1['open'], 150.0)
            self.assertEqual(bar1['high'], 155.0)
            self.assertEqual(bar1['low'], 149.0)
            self.assertEqual(bar1['close'], 154.0)
            self.assertEqual(bar1['volume'], 1000000)
            
            # Check timestamp parsing
            expected_timestamp = datetime(2024, 1, 1, 9, 30, 0)
            self.assertEqual(bar1['timestamp'], expected_timestamp)
    
    @patch('src.alpaca_connector.requests.get')
    def test_validate_connection_success(self, mock_get):
        """Test successful connection validation."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_get.return_value = mock_response
        
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret
        )
        
        result = connector.validate_connection()
        self.assertTrue(result)
    
    @patch('src.alpaca_connector.requests.get')
    def test_validate_connection_failure(self, mock_get):
        """Test failed connection validation."""
        mock_response = Mock()
        mock_response.status_code = 401
        mock_get.return_value = mock_response
        
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret
        )
        
        result = connector.validate_connection()
        self.assertFalse(result)
    
    def test_get_historical_bars_integration(self):
        """Integration test for get_historical_bars method."""
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret,
            config={'date_split_days': 1}  # Force multiple tasks
        )
        
        # Mock the fetch method to return sample data
        def mock_fetch(*args):
            return [
                {
                    'symbol': args[0],
                    'timestamp': datetime(2024, 1, 1, 9, 30),
                    'open': 150.0,
                    'high': 155.0,
                    'low': 149.0,
                    'close': 154.0,
                    'volume': 1000000,
                    'trade_count': 5000,
                    'vwap': 152.5
                }
            ]
        
        with patch.object(connector, '_fetch_bars_for_range', side_effect=mock_fetch):
            df = connector.get_historical_bars(['AAPL'], '2024-01-01', '2024-01-02')
            
            # Verify DataFrame structure
            self.assertEqual(len(df.columns), 9)
            self.assertIn('symbol', df.columns)
            self.assertIn('timestamp', df.columns)
            self.assertIn('open', df.columns)
            
            # Verify data
            rows = df.collect()
            self.assertGreater(len(rows), 0)
            self.assertEqual(rows[0]['symbol'], 'AAPL')
    
    def test_create_connector_function(self):
        """Test the convenience create_connector function."""
        connector = create_connector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret,
            page_size=5000
        )
        
        self.assertIsInstance(connector, AlpacaHistoricalBarsConnector)
        self.assertEqual(connector.api_key, self.api_key)
        self.assertEqual(connector.config['page_size'], 5000)
    
    def test_schema_definition(self):
        """Test that the schema is properly defined."""
        expected_fields = [
            'symbol', 'timestamp', 'open', 'high', 'low', 
            'close', 'volume', 'trade_count', 'vwap'
        ]
        
        schema_fields = [field.name for field in AlpacaHistoricalBarsConnector.BARS_SCHEMA.fields]
        
        for field in expected_fields:
            self.assertIn(field, schema_fields)
    
    def test_empty_response_handling(self):
        """Test handling of empty API responses."""
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key=self.api_key,
            api_secret=self.api_secret
        )
        
        with patch.object(connector, '_fetch_bars_for_range', return_value=[]):
            df = connector.get_historical_bars(['AAPL'], '2024-01-01', '2024-01-02')
            
            self.assertEqual(df.count(), 0)
            self.assertEqual(len(df.columns), 9)


class TestIntegrationScenarios(unittest.TestCase):
    """Integration test scenarios."""
    
    @classmethod
    def setUpClass(cls):
        """Set up SparkSession."""
        cls.spark = SparkSession.builder \
            .appName("AlpacaConnectorIntegrationTests") \
            .master("local[2]") \
            .getOrCreate()
        cls.spark.sparkContext.setLogLevel("WARN")
    
    @classmethod
    def tearDownClass(cls):
        """Tear down SparkSession."""
        cls.spark.stop()
    
    def test_multiple_symbols_and_ranges(self):
        """Test fetching data for multiple symbols and date ranges."""
        connector = AlpacaHistoricalBarsConnector(
            self.spark,
            api_key="test_key",
            api_secret="test_secret",
            config={'date_split_days': 5}
        )
        
        # Mock data for different symbols
        def mock_fetch(symbol, start_date, end_date, timeframe):
            return [
                {
                    'symbol': symbol,
                    'timestamp': datetime.strptime(start_date, '%Y-%m-%d'),
                    'open': 100.0 if symbol == 'AAPL' else 200.0,
                    'high': 105.0 if symbol == 'AAPL' else 210.0,
                    'low': 95.0 if symbol == 'AAPL' else 190.0,
                    'close': 102.0 if symbol == 'AAPL' else 205.0,
                    'volume': 1000000,
                    'trade_count': 5000,
                    'vwap': 101.0 if symbol == 'AAPL' else 202.0
                }
            ]
        
        with patch.object(connector, '_fetch_bars_for_range', side_effect=mock_fetch):
            df = connector.get_historical_bars(
                ['AAPL', 'GOOGL'], 
                '2024-01-01', 
                '2024-01-15'
            )
            
            # Should have data for both symbols
            symbols = [row['symbol'] for row in df.select('symbol').distinct().collect()]
            self.assertIn('AAPL', symbols)
            self.assertIn('GOOGL', symbols)
            
            # Verify different prices for different symbols
            aapl_data = df.filter(df.symbol == 'AAPL').collect()
            googl_data = df.filter(df.symbol == 'GOOGL').collect()
            
            self.assertEqual(aapl_data[0]['open'], 100.0)
            self.assertEqual(googl_data[0]['open'], 200.0)


if __name__ == '__main__':
    # Run tests
    unittest.main(verbosity=2)